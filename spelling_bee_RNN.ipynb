{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Spelling bee challenge\n",
    "\n",
    "In this notebook we are going to try and learn a model that can take in the pronunciation of a word as a list of phonemes, and try to spell it.  On the one hand this is easier than something like speech to text.  However one difficulty is this model will be evaluated on how well it can spell words that it has never seen before.  This is not a completely well-posed question, as there are often several reasonable spellings, and indeed, some words have [homonyms](https://en.wikipedia.org/wiki/Homonym) with different spellings.  I hope in future to try and give the model more information (e.g. country of origin), as in a real spelling bee to see if it can improve its guesses.  \n",
    "\n",
    "## Data Stuff (not interesting)\n",
    "\n",
    "We take our data set from [The CMU pronouncing dictionary](https://en.wikipedia.org/wiki/CMU_Pronouncing_Dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example line of file : \n",
      "SPELLING  S P EH1 L IH0 NG\n"
     ]
    }
   ],
   "source": [
    "cmu_dict_raw = open(\"cmudict-0.7b\").read()\n",
    "\n",
    "first_line = \"A  AH0\"\n",
    "last_line = \"ZYWICKI  Z IH0 W IH1 K IY0\"\n",
    "\n",
    "lines = cmu_dict_raw.split(\"\\n\")\n",
    "\n",
    "for i, l in enumerate(lines):\n",
    "    if l == first_line:\n",
    "        first_index = i\n",
    "    if l == last_line:\n",
    "        last_index = i\n",
    "        \n",
    "print \"Example line of file : \"\n",
    "print lines[113108]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get dictionaries to convert between indexes and letters/phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phonemes = set()\n",
    "\n",
    "for l in lines[first_index : last_index + 1]:\n",
    "    word, pronounce = l.split(\"  \")\n",
    "    for phoneme in pronounce.split():\n",
    "        phonemes.add(phoneme)\n",
    "        \n",
    "sorted_phonemes = [\"_\"] + sorted(list(phonemes))\n",
    "\n",
    "index_to_phoneme = dict(enumerate(sorted_phonemes))\n",
    "phoneme_to_index = dict((v, k) for k,v in index_to_phoneme.items())\n",
    "\n",
    "index_to_letter = dict(enumerate(\"_abcdefghijklmnopqrstuvwxyz\"))\n",
    "letter_to_index = dict((v, k) for k,v in index_to_letter.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "pronounce_dict = {}\n",
    "\n",
    "for l in lines[first_index : last_index + 1]:\n",
    "    word, phone_list = l.split(\"  \")\n",
    "    pronounce_dict[word.lower()] = [phoneme_to_index[p] for p in phone_list.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biggest word in dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_k = max([len(k) for k,v in pronounce_dict.items()])\n",
    "max_v = max([len(v) for k,v in pronounce_dict.items()])\n",
    "for k,v in pronounce_dict.items():\n",
    "    if len(k) == max_k or  len(v) == max_v:\n",
    "        print k\n",
    "        print v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get rid of words that are too long, or that have punctuation or spaces in them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133779\n",
      "108006\n"
     ]
    }
   ],
   "source": [
    "bad_ct = set()\n",
    "\n",
    "letters = set(\"abcdefghijklmnopqrstuvwxyz\")\n",
    "print len(pronounce_dict)\n",
    "for k, v in pronounce_dict.items():\n",
    "    if len(k) < 5 or len(k) > 15:\n",
    "        del pronounce_dict[k]\n",
    "        continue\n",
    "    for c in k:\n",
    "        if c not in letters:\n",
    "            del pronounce_dict[k]\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split lines into words, phonemes, convert to indexes (with padding), split into training, validation, test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "pairs = np.random.permutation(list(pronounce_dict.keys()))\n",
    "\n",
    "input_ = np.zeros((len(pairs), 16))\n",
    "labels_ = np.zeros((len(pairs), 15))\n",
    "\n",
    "for i, k in enumerate(pairs):\n",
    "    v = pronounce_dict[k]\n",
    "    k = k + \"_\" * (15 - len(k))\n",
    "    v = v + [0] * (16 - len(v))\n",
    "    for j, n in enumerate(v):\n",
    "        input_[i][j] = n\n",
    "    for j, letter in enumerate(k):\n",
    "        labels_[i][j] = letter_to_index[letter]\n",
    "        \n",
    "input_ = input_.astype(np.int32)\n",
    "labels_ = labels_.astype(np.int32)\n",
    "\n",
    "input_test   = input_[:10000]\n",
    "input_val    = input_[10000:20000]\n",
    "input_train  = input_[20000:]\n",
    "labels_test  = labels_[:10000]\n",
    "labels_val   = labels_[10000:20000]\n",
    "labels_train = labels_[20000:]\n",
    "\n",
    "data_test  = zip(input_test, labels_test)\n",
    "data_val   = zip(input_val, labels_val)\n",
    "data_train = zip(input_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Tensorflow code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.models.rnn import rnn_cell, seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell resets the graphs and session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ops.reset_default_graph()\n",
    "try:\n",
    "    sess.close()\n",
    "except:\n",
    "    \n",
    "    pass\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_seq_length = 16\n",
    "output_seq_length = 15\n",
    "batch_size = 128\n",
    "\n",
    "input_vocab_size = 70\n",
    "output_vocab_size = 28\n",
    "embedding_dim = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As on this page we take our Seq2Seq learner to have the follwing shape:\n",
    "\n",
    "![alt text](https://www.tensorflow.org/versions/r0.7/images/basic_seq2seq.png \"Seq2Seq\")\n",
    "\n",
    "This means the decode_input has to be shifted along by one from the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encode_input = [tf.placeholder(tf.int32, \n",
    "                                shape=(None,),\n",
    "                                name = \"ei_%i\" %i)\n",
    "                                for i in range(input_seq_length)]\n",
    "\n",
    "labels = [tf.placeholder(tf.int32,\n",
    "                                shape=(None,),\n",
    "                                name = \"l_%i\" %i)\n",
    "                                for i in range(output_seq_length)]\n",
    "\n",
    "decode_input = [tf.zeros_like(encode_input[0], dtype=np.int32, name=\"GO\")] + labels[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell is the meat of the model, and a lot is happening here under the hood.  We take our cells to be LSTM recurrent units, with dropout between the feed-forward layers.  We take 3 of these stacked as our neural network.  We then run this using the seq2seq.embedding_rnn_seq2seq pattern - this let's us hand the neural network sequences like 1,2,3,2,1 - and the neural network automatically embeds this as a one-hot tensor for us.  \n",
    "\n",
    "Note that we build two networks within the 'decoders' scope.  One of these is using feed_previous = True, the other not.  We set this to False during training, so that even if the learner makes a mistake on a letter - we still give it the correct label in the decoder_inputs.  Since we don't have the real label for the test set, this is set to True, and the decoder takes the letter with maximum probability from the last step of the decoder output.  \n",
    "\n",
    "The decode_output is a tensor of shape (batch_size, output_vocab_size).  We can run softmax on this to get logit scores for each letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cells = [rnn_cell.DropoutWrapper(\n",
    "        rnn_cell.BasicLSTMCell(embedding_dim), output_keep_prob=0.5\n",
    "    ) for i in range(3)]\n",
    "\n",
    "stacked_lstm = rnn_cell.MultiRNNCell(cells)\n",
    "\n",
    "with tf.variable_scope(\"decoders\") as scope:\n",
    "    decode_outputs, decode_state = seq2seq.embedding_rnn_seq2seq(\n",
    "        encode_input, decode_input, cell, input_vocab_size, output_vocab_size)\n",
    "    \n",
    "    scope.reuse_variables()\n",
    "    \n",
    "    decode_outputs_test, decode_state_test = seq2seq.embedding_rnn_seq2seq(\n",
    "        encode_input, decode_input, cell, input_vocab_size, output_vocab_size, \n",
    "    feed_previous=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sequence_loss is cross-entropy on the soft max of the decode outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_weights = [tf.ones_like(l, dtype=tf.float32) for l in labels]\n",
    "loss = seq2seq.sequence_loss(decode_outputs, labels, loss_weights, output_vocab_size)\n",
    "optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple class for getting random batches and reshaping them properly for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataIterator:\n",
    "    def __init__(self, data, batch_size):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.iter = self.make_random_iter()\n",
    "        \n",
    "    def next_batch(self):\n",
    "        try:\n",
    "            idxs = self.iter.next()\n",
    "        except StopIteration:\n",
    "            self.iter = self.make_random_iter()\n",
    "            idxs = self.iter.next()\n",
    "        X, Y = zip(*[self.data[i] for i in idxs])\n",
    "        X = np.array(X).T\n",
    "        Y = np.array(Y).T\n",
    "        return X, Y\n",
    "\n",
    "    def make_random_iter(self):\n",
    "        splits = np.arange(self.batch_size, len(self.data), self.batch_size)\n",
    "        it = np.split(np.random.permutation(range(len(self.data))), splits)[:-1]\n",
    "        return iter(it)\n",
    "    \n",
    "train_iter = DataIterator(data_train, 128)\n",
    "val_iter = DataIterator(data_val, 128)\n",
    "test_iter = DataIterator(data_test, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our evaluation scores are based on the seq2seq loss, and on the precision - the number of words that the model spells perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def get_feed(X, Y):\n",
    "    feed_dict = {encode_input[t]: X[t] for t in range(input_seq_length)}\n",
    "    feed_dict.update({labels[t]: Y[t] for t in range(output_seq_length)})\n",
    "    return feed_dict\n",
    "\n",
    "def train_batch(data_iter):\n",
    "    X, Y = data_iter.next_batch()\n",
    "    feed_dict = get_feed(X, Y)\n",
    "    _, out = sess.run([train_op, loss], feed_dict)\n",
    "    return out\n",
    "\n",
    "def get_eval_batch_data(data_iter):\n",
    "    X, Y = data_iter.next_batch()\n",
    "    feed_dict = get_feed(X, Y)\n",
    "    all_output = sess.run([loss] + decode_outputs_test, feed_dict)\n",
    "    eval_loss = all_output[0]\n",
    "    decode_output = np.array(all_output[1:]).transpose([1,0,2])\n",
    "    return eval_loss, decode_output, X, Y\n",
    "\n",
    "def eval_batch(data_iter, num_batches):\n",
    "    losses = []\n",
    "    predict_loss = []\n",
    "    for i in range(num_batches):\n",
    "        eval_loss, output, X, Y = get_eval_batch_data(data_iter)\n",
    "        losses.append(eval_loss)\n",
    "        \n",
    "        for index in range(len(output)):\n",
    "            real = Y.T[index]\n",
    "            predict = np.argmax(output, axis = 2)[index]\n",
    "            predict_loss.append(all(real==predict))\n",
    "    return np.mean(losses), np.mean(predict_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I simply ran this until it looked like the validation set score was not improving then aborted with a keyboard interrupt.  This took about 20 minutes on my Titan X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss : 0.200953, val predict = 40.0%\n",
      "train loss : 0.165016, train predict = 48.0%\n",
      "\n",
      "val loss : 0.187188, val predict = 42.5%\n",
      "train loss : 0.143054, train predict = 50.6%\n",
      "\n",
      "val loss : 0.175085, val predict = 45.8%\n",
      "train loss : 0.115830, train predict = 56.4%\n",
      "\n",
      "val loss : 0.178171, val predict = 45.7%\n",
      "train loss : 0.111954, train predict = 58.9%\n",
      "\n",
      "val loss : 0.172554, val predict = 49.3%\n",
      "train loss : 0.091568, train predict = 65.0%\n",
      "\n",
      "val loss : 0.181853, val predict = 46.6%\n",
      "train loss : 0.083485, train predict = 66.0%\n",
      "\n",
      "val loss : 0.179023, val predict = 48.3%\n",
      "train loss : 0.064898, train predict = 72.2%\n",
      "\n",
      "interrupted by user\n"
     ]
    }
   ],
   "source": [
    "for i in range(10000):\n",
    "    try:\n",
    "        train_batch(train_iter)\n",
    "        if i % 1000 == 0:\n",
    "            val_loss, val_predict = eval_batch(val_iter, 16)\n",
    "            train_loss, train_predict = eval_batch(train_iter, 16)\n",
    "            print \"val loss   : %f, val predict   = %.1f%%\" %(val_loss, val_predict * 100)\n",
    "            print \"train loss : %f, train predict = %.1f%%\" %(train_loss, train_predict * 100)\n",
    "            print\n",
    "            sys.stdout.flush()\n",
    "    except KeyboardInterrupt:\n",
    "        print \"interrupted by user\"\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Examining model outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reach ~ 50% on our hold-out validation set, which may seem low.  Let's see some of the outputs on our test data set, to see the kind of mistakes that the model is making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_loss, output, X, Y = get_eval_batch_data(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pronunciation                            real spelling     model spelling    is correct\n",
      "\n",
      "HH-EH1-P-N-ER0                           hepner            hepner            True\n",
      "P-AA1-M-D-EY2-L                          palmdale          pomdale           False\n",
      "B-AY1-S-ER0                              beiser            beiser            True\n",
      "G-W-AA1-V-AH0                            guava             guava             True\n",
      "M-AO1-N-D-ER0-IH0-NG-Z                   maunderings       monderings        False\n",
      "R-IH1-M-AH0-L                            rimel             rimmel            False\n",
      "SH-UW0-V-R-AA1-N-T                       cheuvront         shovrant          False\n",
      "L-IH1-Z-AH0-K                            lizak             lizac             False\n",
      "B-R-AY1-B-Z                              bribes            bribes            True\n",
      "K-AE1-L-AH0-B-ER0                        caliber           caliber           True\n",
      "W-IY2-D-ER0-AO1-F-B-AW0                  wiederaufbau      weiderafbu        False\n",
      "JH-EY1-K-AH0-B-S-AH0-N                   jacobson          jacobsen          False\n",
      "L-AH0-N-D-AO1-F-S-K-IY0                  landowski         landowski         True\n",
      "SH-AE1-N-T-IY0-T-AW2-N-Z                 shantytowns       shantitowns       False\n",
      "Y-ER1-K-AH0-V-IH0-CH                     jurkovich         yurkovich         False\n",
      "AA0-D-OW1-L-F-AH0                        adolpha           aodlopha          False\n",
      "HH-EY1-N-ER0                             haner             hayner            False\n",
      "AE2-K-T-AH0-V-EY1-SH-AH0-N               activation        activation        True\n",
      "N-AA0-N-EH1-N-T-AH0-T-IY0                nonentity         nonnetity         False\n",
      "HH-AE1-NG-ER0-Z                          hangers           hangars           False\n",
      "HH-AY1-D-EY1-AH0                         hydeia            hydea             False\n",
      "F-AO1-R-S-AH0-B-AH0-L                    forcible          forciball         False\n",
      "T-EH1-M-P-ER0                            temper            temper            True\n",
      "B-EY1-L-IH0-F                            bayliff           bailiff           False\n",
      "S-K-W-AA1-N-D-ER0-Z                      squanders         squanders         True\n",
      "P-L-AE1-S-T-IH0-S-IY2-N                  plasticine        plasticine        True\n",
      "AH0-T-EH1-N-SH-AH0-N-Z                   attentions        attentions        True\n",
      "M-EH1-R-IY0-ER0                          marier            merrier           False\n",
      "K-IY1-AH0                                cleah             keeha             False\n",
      "AE1-M-ER0-IH0-S                          amaris            amaris            True\n",
      "T-ER0-AA1-N-OW2                          terrano           teronno           False\n",
      "T-W-IH1-NG-K-AH0-L-D                     twinkled          twinkled          True\n",
      "F-EH1-L-P-S                              phelps            felps             False\n",
      "T-R-AA1-P                                tropp             troppe            False\n",
      "T-R-AH0-SH-K-AO1-F-S-K-IY0               truszkowski       traszkowski       False\n",
      "SH-AA1-TH                                shaath            schoth            False\n",
      "F-AO0-R-G-OW1-IH0-NG                     forgoing          forgoing          True\n",
      "S-IH1-S-T-R-AH0-NG-K                     sistrunk          sistrunk          True\n",
      "SH-ER0-IY1                               sheree            sharee            False\n",
      "K-AW2-N-AH1-K-AH0-K-AY0                  kaunakakai        councouchie       False\n",
      "HH-AA1-JH-AH0-N-Z                        hodgens           hodgens           True\n",
      "L-AA1-P-IH0-NG                           lopping           lopping           True\n",
      "AH0-T-R-AE1-K-T                          attract           attract           True\n",
      "L-UW1-D-K-IY0                            luedke            ludke             False\n",
      "Z-W-IY1-B-ER0                            zweber            zwieber           False\n",
      "V-EH1-L-D-M-AH0-N                        veldman           veldman           True\n",
      "D-EH1-M-ER0-S                            demerse           demerse           True\n",
      "D-IH0-L-AY1-L                            delisle           delile            False\n",
      "P-AH2-NG-K-S-AH0-T-AW1-N-IY2             punxsutawney      punkistowny       False\n",
      "M-IH1-S-T-ER0-Z                          misters           misters           True\n",
      "ZH-AE1-NG                                zhang             jaing             False\n",
      "W-IH1-N-D-SH-IY2-L-D                     windshield        windshield        True\n",
      "K-OW1-K-AH0-M-OW2                        kokomo            cocomo            False\n",
      "S-T-AA1-R-D-AH0-M                        stardom           stardom           True\n",
      "R-AE1-M-IY0                              ramey             rammy             False\n",
      "IH0-M-AE1-S-OW0                          imasco            emasso            False\n",
      "S-AE1-L-S-G-IH0-V-ER0                    salsgiver         salsgiver         True\n",
      "K-AA1-N-JH-ER0-D                         conjured          conjured          True\n",
      "V-OY2-Y-UW1-R-IH0-Z-AH0-M                voyeurism         voyeurism         True\n",
      "IH2-N-T-R-AE1-V-AH0-N-AH0-S-L-IY0        intravenously     intravonously     False\n",
      "D-IH0-G-R-EY1-D-AH0-B-AH0-L              degradable        degradable        True\n",
      "T-AH1-S-T-IH0-N                          tustin            tustin            True\n",
      "K-AH0-L-AH1-M-B-IY0-AH0                  colombia          colombia          True\n",
      "IH1-N-F-AH0-N-T-R-IY0                    infantry          infentry          False\n",
      "R-EH2-M-AH0-N-IH1-S-AH0-N-T              reminiscent       reminiscent       True\n",
      "D-EH1-M-AH0-N-S-T-R-EY2-T-IH0-NG         demonstrating     demonstrating     True\n",
      "K-AH0-N-S-ER1-V-AH0-T-IH0-V-Z            conservatives     conservatives     True\n",
      "P-OW1-S-K-R-IH2-P-T                      postscript        poscript          False\n",
      "K-W-IH1-K-AH0-N-D                        quickened         quickened         True\n",
      "K-AH0-N-S-T-AE1-B-Y-AH0-L-EH2-R-IY0      constabulary      constabulary      True\n",
      "B-UW0-L-EY1                              boulais           boulay            False\n",
      "D-EH1-W-ER0-S-T                          dewhirst          dewhurst          False\n",
      "S-K-AA0-T-UH1-R-OW0                      scaturro          scaturo           False\n",
      "JH-AY1-L-Z                               giles             jiles             False\n",
      "K-AH0-N-K-AE1-T-AH0-N-EY2-T-AH0-D        concatenated      concatinated      False\n",
      "S-M-IH2-DH-ER0-IY1-N                     smithereen        smithereen        True\n",
      "R-AY1-S-D-AO0-R-F                        reisdorf          risedorf          False\n",
      "K-AO1-R-K-ER0                            corker            corker            True\n",
      "K-EH2-R-AH0-L-AY1-N-AH0                  karolina          carolina          False\n",
      "R-EH1-D-L-ER0                            redler            redler            True\n",
      "K-R-AO1-S-B-OW2                          crossbow          crosbown          False\n",
      "JH-IH0-B-R-AO1-L-T-ER0                   gibraltar         gibraltor         False\n",
      "S-ER0-AE1-N                              saran             suran             False\n",
      "K-AE1-S-AH0-B-AW2-M                      kassebaum         casabeaum         False\n",
      "M-IY2-T-IY0-ER0-AA1-L-AH0-JH-IH0-S-T     meteorologist     meteorologist     True\n",
      "G-AA1-UW0-IY0                            goewey            goohee            False\n",
      "G-AH1-V-ER0-M-EH2-N-T-AH0-L              governmental      governmental      True\n",
      "B-AA1-S-AH0-L                            basile            bossill           False\n",
      "D-ER1-K-S-AH0-N                          derksen           dirkson           False\n",
      "IH1-N-K-AH2-M-Z                          incomes           incomes           True\n",
      "D-AH0-P-AA1-Z-IH0-T-IH0-NG               depositing        depositing        True\n",
      "D-IH0-L-IH1-N-IY0-EY2-T-IH0-D            delineated        delineated        True\n",
      "R-OW0-M-AE1-N-S-IH0-Z                    romances          romances          True\n",
      "V-IH0-K-T-AO1-R-IY0-AH0                  victoria          victoria          True\n",
      "F-ER1-L-AO2-NG                           furlong           ferlang           False\n",
      "P-R-OW1-M-IH0-S-K-W-AH0-S                promiscuous       promisquous       False\n",
      "B-AH0-G-D-AE1-N-AH0-W-IH2-CH             bogdanowicz       bogdanathique     False\n",
      "HH-EH1-D-W-EY2                           headway           headway           True\n",
      "G-EH1-R-IH0-TH                           gareth            gareth            True\n",
      "K-AA0-N-S-IY1-G-L-IY0-OW0                consiglio         consiglio         True\n",
      "K-R-AH1-M-IH0-T                          crummett          crummitt          False\n",
      "OW1-B-ER0-HH-OW0-L-T-Z-ER0               oberholtzer       oberholtzer       True\n",
      "M-IH1-L-K-S                              milks             milks             True\n",
      "W-AA1-L-AH0-N-IH0-N                      wolanin           wollanin          False\n",
      "IH2-N-Y-UW0-EH1-N-D-OW0-Z                innuendoes        innuendos         False\n",
      "G-IH1-G-AH0-L-D                          giggled           giggled           True\n",
      "D-AE1-N-Y-AH0-L-S-AH0-N                  danielson         danulson          False\n",
      "B-AA1-R-T-N-IH0-K                        bartnick          bartnik           False\n",
      "N-OW0-EH1-L                              noelle            noelle            True\n",
      "F-EH1-R-AH0-N                            ferran            ferron            False\n",
      "AE2-B-S-AH0-L-UW1-SH-AH0-N               absolution        absolustion       False\n",
      "G-L-AO1-S-IY0-ER0                        glossier          glossier          True\n",
      "S-IH0-L-EH1-N-T-OW2                      cilento           cilento           True\n",
      "AE0-K-S-EH1-P-T-AH0-B-L-IY0              acceptably        acceptably        True\n",
      "G-R-IY1-N-W-UH2-D                        greenwood         greenwood         True\n",
      "AE2-L-AH0-M-IY1-T-OW0-S                  alamitos          alamitos          True\n",
      "P-R-IH0-M-AO1-R-D-IY0-AH0-L              primordial        premordial        False\n",
      "K-R-AE1-CH                               krach             cratch            False\n",
      "W-AY1-T-S-AY2-D-Z                        whitesides        whitesides        True\n",
      "JH-OW1-B-Z                               jobes             joebs             False\n",
      "K-L-AE1-S-IH0-NG                         classing          classing          True\n",
      "S-T-AE1-G-ER0-IH2-NG                     staggering        staggering        True\n",
      "TH-R-AA0-M-B-OW1-L-IH0-S-IH0-S           thrombolysis      thromboloiss      False\n",
      "D-EH0-S-AE1-NG-K-T-IH0-S                 desanctis         desanctis         True\n",
      "M-IH2-S-D-AH0-M-IY1-N-ER0-Z              misdemeanors      misdomeners       False\n",
      "G-L-AA1-N-AH0-S                          glonass           glonness          False\n",
      "F-IH1-L-IH0-NG                           filling           filling           True\n",
      "K-OW2-AH0-L-EH1-S-IH0-Z                  coalesces         coalesces         True\n"
     ]
    }
   ],
   "source": [
    "print \"pronunciation\".ljust(40),\n",
    "print \"real spelling\".ljust(17),\n",
    "print \"model spelling\".ljust(17),\n",
    "print \"is correct\"\n",
    "print\n",
    "\n",
    "for index in range(len(output)):\n",
    "    phonemes = \"-\".join([index_to_phoneme[p] for p in X.T[index]]) \n",
    "    real = [index_to_letter[l] for l in Y.T[index]] \n",
    "    predict = [index_to_letter[l] for l in np.argmax(output, axis = 2)[index]]\n",
    "   \n",
    "    print phonemes.split(\"-_\")[0].ljust(40),\n",
    "    print \"\".join(real).split(\"_\")[0].ljust(17),\n",
    "    print \"\".join(predict).split(\"_\")[0].ljust(17),\n",
    "    print str(real == predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
